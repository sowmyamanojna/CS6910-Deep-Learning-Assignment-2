{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part-B.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPMpHADV26Jbrq8pujmdQm9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bcadf3c0a38b402cac1ee70b3bc9db25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_074be0ac6163479f808e46a0d0b90bf2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cf802ab6790843439f31e89de5e21d40",
              "IPY_MODEL_e28e61bd3f024112a7fbc644f22708b6"
            ]
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LS0fc9-ZrXN"
      },
      "source": [
        "!pip install -q wandb\n",
        "import wandb\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
        "from PIL import Image"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1K34jr47OKyM",
        "outputId": "e9482f83-cb6f-478a-f525-069016f4922c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NwKzzeTZy1y",
        "outputId": "98fc0c0e-ca1e-4012-c8b8-6882612dff75"
      },
      "source": [
        "################################################################\n",
        "# Preparing training (without augmentation) and validation set \n",
        "################################################################\n",
        "# Preparing training and validation sets without augmentation\n",
        "# Loading data from directory\n",
        "# data_dir = pathlib.Path('/content/drive/MyDrive/inaturalist_12K/train') # Set path to the right directory\n",
        "data_dir = '/content/drive/MyDrive/inaturalist_12K/train' # Set path to the right directory\n",
        "train_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "                      directory = data_dir,\n",
        "                      labels = 'inferred',  \n",
        "                      label_mode = 'categorical',\n",
        "                      color_mode = 'rgb',\n",
        "                      batch_size = 32,\n",
        "                      image_size = (256, 256),\n",
        "                      shuffle = True,\n",
        "                      seed = 17,\n",
        "                      validation_split = 0.2,\n",
        "                      subset = 'training')\n",
        "\n",
        "val_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "                      directory = data_dir,\n",
        "                      labels = 'inferred',  \n",
        "                      label_mode = 'categorical',\n",
        "                      color_mode = 'rgb',\n",
        "                      batch_size = 32,\n",
        "                      image_size = (256, 256),\n",
        "                      shuffle = True,\n",
        "                      seed = 17,\n",
        "                      validation_split = 0.2,\n",
        "                      subset = 'validation')\n",
        "\n",
        "# Retaining 25 percent of train and validation data and discarding the rest\n",
        "len_train, len_val = len(train_data), len(val_data)\n",
        "train_data = train_data.take(int(0.25*len_train))\n",
        "val_data = val_data.take(int(0.25*len_val))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 10000 files belonging to 10 classes.\n",
            "Using 8000 files for training.\n",
            "Found 10000 files belonging to 10 classes.\n",
            "Using 2000 files for validation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6qajDpNZz-B",
        "outputId": "d5bf298e-6d3e-46bf-f0fb-aff078e0eb67"
      },
      "source": [
        "################################################################\n",
        "# Preparing training set with augmentation \n",
        "################################################################\n",
        "train_data_augmenter = ImageDataGenerator(\n",
        "                            rescale = None,\n",
        "                            rotation_range = 20,\n",
        "                            width_shift_range = 0.2,\n",
        "                            height_shift_range = 0.2,\n",
        "                            brightness_range = [0.2, 1.5],\n",
        "                            shear_range = 0.2,\n",
        "                            zoom_range = 0.2,\n",
        "                            horizontal_flip=True,\n",
        "                            data_format = 'channels_last',\n",
        "                            validation_split = 0.2)        #Specifying parameters for augmentation of training data\n",
        "\n",
        "val_data_augmenter = ImageDataGenerator(validation_split = 0.2) #No augmentation of validation data\n",
        "\n",
        "train_aug_gen = train_data_augmenter.flow_from_directory(data_dir, shuffle = True, \\\n",
        "                                                         seed = 17, subset = 'training')\n",
        "val_aug_gen = val_data_augmenter.flow_from_directory(data_dir, shuffle = True, \\\n",
        "                                                     seed = 17, subset = 'validation')\n",
        "\n",
        "train_aug_data = tf.data.Dataset.from_generator(\n",
        "                    lambda: train_aug_gen,\n",
        "                    output_types = (tf.float32, tf.float32),\n",
        "                    output_shapes = ([None, 256, 256, 3], [None, 10]))\n",
        "\n",
        "val_aug_data = tf.data.Dataset.from_generator(\n",
        "                  lambda: val_aug_gen,\n",
        "                  output_types = (tf.float32, tf.float32),\n",
        "                  output_shapes = ([None, 256, 256, 3], [None, 10]))\n",
        "\n",
        "# Retaining 25 percent of train and validation data and discarding the rest\n",
        "train_aug_data = train_aug_data.take(int(0.25*len_train))\n",
        "val_aug_data = val_aug_data.take(int(0.25*len_val))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8000 images belonging to 10 classes.\n",
            "Found 2000 images belonging to 10 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS0Zq3NrUiCr"
      },
      "source": [
        "Using tutorials:\n",
        "- https://www.tensorflow.org/tutorials/text/image_captioning\n",
        "- https://www.tensorflow.org/tutorials/images/transfer_learning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z79mSMMEZ8xn"
      },
      "source": [
        "class CNN(Model):\n",
        "    def __init__(self, base_model_name, tune=False, offset=20):\n",
        "        super(CNN, self).__init__()\n",
        "        self.IMG_SHAPE = (256, 256, 3)\n",
        "        self.base_model_name = base_model_name\n",
        "        self.init_base_model()\n",
        "        \n",
        "        if tune:\n",
        "            self.base_model.trainable = True\n",
        "            fine_tune_at = len(self.base_model.layers)-offset\n",
        "            # Freeze all the layers before the `fine_tune_at` layer\n",
        "            for layer in self.base_model.layers[:fine_tune_at]:\n",
        "                layer.trainable =  False\n",
        "        else:\n",
        "            self.base_model.trainable = False\n",
        "\n",
        "        self.additional_avg_pool1 = layers.GlobalAveragePooling2D()\n",
        "        self.additional_dense1 = layers.Dense(10)\n",
        "\n",
        "    def init_base_model(self):\n",
        "        if self.base_model_name == \"InceptionV3\":\n",
        "            self.base_model = tf.keras.applications.InceptionV3(input_shape=self.IMG_SHAPE,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "        elif self.base_model_name == \"InceptionResNetV2\":\n",
        "            self.base_model = tf.keras.applications.InceptionResNetV2(input_shape=self.IMG_SHAPE,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "        elif self.base_model_name == \"ResNet50\":\n",
        "            self.base_model = tf.keras.applications.ResNet50(input_shape=self.IMG_SHAPE,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "        elif self.base_model_name == \"Xception\":\n",
        "            self.base_model = tf.keras.applications.Xception(input_shape=self.IMG_SHAPE,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "            \n",
        "    def call(self, inputs):\n",
        "        x = self.base_model(inputs)\n",
        "        x = self.additional_avg_pool1(x)\n",
        "        return self.additional_dense1(x)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCTRL1QdOGxX"
      },
      "source": [
        "###############################################\n",
        "# Listing the hyperparameters in wandb config \n",
        "###############################################\n",
        "sweep_config = {'name': 'random-test-sweep', 'method': 'grid'}\n",
        "sweep_config['metric'] = {'name': 'val_acc', 'goal': 'maximize'}\n",
        "parameters_dict = {\n",
        "                   'base_model_name': {'values': [\"InceptionV3\", \"InceptionResNetV2\", \"ResNet50\", \"Xception\"]},\n",
        "                   'tune': {'values': [False, True]},\n",
        "                  }\n",
        "sweep_config['parameters'] = parameters_dict"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA845WJNRizR"
      },
      "source": [
        "def pretrain_CNN_sweep(config=sweep_config):\n",
        "    with wandb.init(config=config):\n",
        "        config = wandb.init().config\n",
        "        wandb.run.name = 'BM_{}_tune_{}'.format(config.base_model_name, \\\n",
        "                                                          config.tune)\n",
        "        \n",
        "        model = CNN(config.base_model_name, config.tune)\n",
        "        base_learning_rate = 0.0001\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
        "                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "        \n",
        "        history = model.fit(train_data, epochs=10, validation_data=val_data, \\\n",
        "                            callbacks = [wandb.keras.WandbCallback()])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905,
          "referenced_widgets": [
            "bcadf3c0a38b402cac1ee70b3bc9db25"
          ]
        },
        "id": "QN0MMuvoyOqD",
        "outputId": "c00e2373-26e6-4c48-f6e6-d7cd2c66805e"
      },
      "source": [
        "#################################\n",
        "# Setting up wandb sweeps\n",
        "#################################\n",
        "sweep_id = wandb.sweep(sweep_config, project = 'DL-Assignment2-PartB-9April')\n",
        "wandb.agent(sweep_id, function=pretrain_CNN_sweep)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: rym5zond\n",
            "Sweep URL: https://wandb.ai/sowmyamanojna/DL-Assignment2-PartB-9April/sweeps/rym5zond\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ydnoo4aj with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_model_name: InceptionV3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \ttune: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msowmyamanojna\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.25<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">silvery-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/sowmyamanojna/DL-Assignment2-PartB-9April\" target=\"_blank\">https://wandb.ai/sowmyamanojna/DL-Assignment2-PartB-9April</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/sowmyamanojna/DL-Assignment2-PartB-9April/sweeps/rym5zond\" target=\"_blank\">https://wandb.ai/sowmyamanojna/DL-Assignment2-PartB-9April/sweeps/rym5zond</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/sowmyamanojna/DL-Assignment2-PartB-9April/runs/ydnoo4aj\" target=\"_blank\">https://wandb.ai/sowmyamanojna/DL-Assignment2-PartB-9April/runs/ydnoo4aj</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210409_181808-ydnoo4aj</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Finishing last run (ID:ydnoo4aj) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 1859<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bcadf3c0a38b402cac1ee70b3bc9db25",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210409_181808-ydnoo4aj/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210409_181808-ydnoo4aj/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">silvery-sweep-1</strong>: <a href=\"https://wandb.ai/sowmyamanojna/DL-Assignment2-PartB-9April/runs/ydnoo4aj\" target=\"_blank\">https://wandb.ai/sowmyamanojna/DL-Assignment2-PartB-9April/runs/ydnoo4aj</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "...Successfully finished last run (ID:ydnoo4aj). Initializing new run:<br/><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.25<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">silvery-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/sowmyamanojna/DL-Assignment2-PartB-9April\" target=\"_blank\">https://wandb.ai/sowmyamanojna/DL-Assignment2-PartB-9April</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/sowmyamanojna/DL-Assignment2-PartB-9April/sweeps/rym5zond\" target=\"_blank\">https://wandb.ai/sowmyamanojna/DL-Assignment2-PartB-9April/sweeps/rym5zond</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/sowmyamanojna/DL-Assignment2-PartB-9April/runs/ydnoo4aj\" target=\"_blank\">https://wandb.ai/sowmyamanojna/DL-Assignment2-PartB-9April/runs/ydnoo4aj</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210409_181811-ydnoo4aj</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "62/62 [==============================] - 54s 701ms/step - loss: 18.0064 - accuracy: 0.0855 - val_loss: 9.9351 - val_accuracy: 0.0875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/10\n",
            "62/62 [==============================] - 43s 664ms/step - loss: 9.4341 - accuracy: 0.1057 - val_loss: 8.6453 - val_accuracy: 0.1167\n",
            "Epoch 3/10\n",
            "62/62 [==============================] - 44s 664ms/step - loss: 8.2158 - accuracy: 0.1288 - val_loss: 7.8264 - val_accuracy: 0.1187\n",
            "Epoch 4/10\n",
            "62/62 [==============================] - 44s 664ms/step - loss: 7.3945 - accuracy: 0.1325 - val_loss: 7.2020 - val_accuracy: 0.1250\n",
            "Epoch 5/10\n",
            "62/62 [==============================] - 44s 670ms/step - loss: 6.7160 - accuracy: 0.1509 - val_loss: 7.0768 - val_accuracy: 0.1208\n",
            "Epoch 6/10\n",
            "62/62 [==============================] - 44s 664ms/step - loss: 6.2827 - accuracy: 0.1445 - val_loss: 6.3817 - val_accuracy: 0.1146\n",
            "Epoch 7/10\n",
            "62/62 [==============================] - 44s 664ms/step - loss: 5.9522 - accuracy: 0.1647 - val_loss: 6.3229 - val_accuracy: 0.1208\n",
            "Epoch 8/10\n",
            "62/62 [==============================] - 44s 667ms/step - loss: 5.7501 - accuracy: 0.1724 - val_loss: 6.1239 - val_accuracy: 0.1333\n",
            "Epoch 9/10\n",
            "33/62 [==============>...............] - ETA: 14s - loss: 5.4207 - accuracy: 0.1649"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}